{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dlib in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (19.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyttsx3 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (2.90)\n",
      "Requirement already satisfied: comtypes in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from pyttsx3) (1.2.1)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\alsgu\\appdata\\roaming\\python\\python311\\site-packages (from pyttsx3) (306)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\alsgu\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\alsgu\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.26.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import urllib.request\n",
    "\n",
    "# 다운로드 링크\n",
    "url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "\n",
    "# 파일 다운로드\n",
    "urllib.request.urlretrieve(url, \"shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "\n",
    "# 압축 해제\n",
    "import bz2\n",
    "\n",
    "with open(\"shape_predictor_68_face_landmarks.dat\", \"wb\") as f_out, bz2.open(\"shape_predictor_68_face_landmarks.dat.bz2\", \"rb\") as f_in:\n",
    "    f_out.write(f_in.read())\n",
    "\n",
    "# 파일 닫기\n",
    "f_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 26, 34, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 34, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 13, 17, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 17, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 6, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 6, 8, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 3, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1536)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 880129 (3.36 MB)\n",
      "Trainable params: 880129 (3.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "IMG_SIZE = (34, 26)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "model = load_model('models/2023_12_27_15_42_17.h5')\n",
    "model.summary()\n",
    "\n",
    "def crop_eye(img, eye_points):\n",
    "    x1, y1 = np.amin(eye_points, axis=0)\n",
    "    x2, y2 = np.amax(eye_points, axis=0)\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    w = (x2 - x1) * 1.2\n",
    "    h = w * IMG_SIZE[1] / IMG_SIZE[0]\n",
    "\n",
    "    margin_x, margin_y = w / 2, h / 2\n",
    "\n",
    "    min_x, min_y = int(cx - margin_x), int(cy - margin_y)\n",
    "    max_x, max_y = int(cx + margin_x), int(cy + margin_y)\n",
    "\n",
    "    eye_rect = np.rint([min_x, min_y, max_x, max_y]).astype(int)\n",
    "\n",
    "    eye_img = gray[eye_rect[1]:eye_rect[3], eye_rect[0]:eye_rect[2]]\n",
    "\n",
    "    return eye_img, eye_rect\n",
    "\n",
    "capture = cv2.VideoCapture(0)  # 디바이스 아이디\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 2560)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1440)\n",
    "\n",
    "while True:\n",
    "    ret, img_ori = capture.read()\n",
    "    img = img_ori.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = detector(gray)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        face = faces[0]\n",
    "        shapes = predictor(gray, face)\n",
    "        shapes = face_utils.shape_to_np(shapes)\n",
    "\n",
    "        eye_img_l, eye_rect_l = crop_eye(gray, eye_points=shapes[36:42])\n",
    "        eye_img_r, eye_rect_r = crop_eye(gray, eye_points=shapes[42:48])\n",
    "\n",
    "        eye_img_l = cv2.resize(eye_img_l, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.resize(eye_img_r, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.flip(eye_img_r, flipCode=1)\n",
    "\n",
    "        #cv2.imshow('l', eye_img_l)\n",
    "        #cv2.imshow('r', eye_img_r)\n",
    "\n",
    "        eye_input_l = eye_img_l.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "        eye_input_r = eye_img_r.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "\n",
    "        pred_l = model.predict(eye_input_l)\n",
    "        pred_r = model.predict(eye_input_r)\n",
    "\n",
    "        # visualize\n",
    "        state_l = 'O %.1f' if pred_l > 0.1 else '- %.1f'\n",
    "        state_r = 'O %.1f' if pred_r > 0.1 else '- %.1f'\n",
    "\n",
    "        state_l = state_l % pred_l\n",
    "        state_r = state_r % pred_r\n",
    "\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_l[0:2]), pt2=tuple(eye_rect_l[2:4]), color=(255, 255, 255), thickness=2)\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_r[0:2]), pt2=tuple(eye_rect_r[2:4]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "        cv2.putText(img, state_l, tuple(eye_rect_l[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img, state_r, tuple(eye_rect_r[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('result', img)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # ESC 키를 누르면 종료\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 음성알림 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26, 34, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 34, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 17, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 17, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 8, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               786944    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 880129 (3.36 MB)\n",
      "Trainable params: 880129 (3.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from tensorflow.keras.models import load_model\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "IMG_SIZE = (34, 26)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "model = load_model('models/2024_01_05_14_16_21.h5')\n",
    "model.summary()\n",
    "\n",
    "# Text-to-speech engine 초기화\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)  # 음성 속도 조절\n",
    "\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def crop_eye(img, eye_points):\n",
    "    x1, y1 = np.amin(eye_points, axis=0)\n",
    "    x2, y2 = np.amax(eye_points, axis=0)\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    w = (x2 - x1) * 1.2\n",
    "    h = w * IMG_SIZE[1] / IMG_SIZE[0]\n",
    "\n",
    "    margin_x, margin_y = w / 2, h / 2\n",
    "\n",
    "    min_x, min_y = int(cx - margin_x), int(cy - margin_y)\n",
    "    max_x, max_y = int(cx + margin_x), int(cy + margin_y)\n",
    "\n",
    "    eye_rect = np.rint([min_x, min_y, max_x, max_y]).astype(int)\n",
    "\n",
    "    eye_img = gray[eye_rect[1]:eye_rect[3], eye_rect[0]:eye_rect[2]]\n",
    "\n",
    "    return eye_img, eye_rect\n",
    "\n",
    "capture = cv2.VideoCapture(0)  # 디바이스 아이디\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 2560)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1440)\n",
    "\n",
    "prev_time = time.time()  # 이전에 눈을 감은 시간\n",
    "\n",
    "# 눈 깜빡임 주기 확인을 위한 변수들\n",
    "blink_threshold = 3  # 눈을 깜빡이지 않아도 작동할 시간\n",
    "blink_counter = 0    # 눈을 깜빡인 시간을 측정하는 카운터\n",
    "\n",
    "while True:\n",
    "    ret, img_ori = capture.read()\n",
    "    img = img_ori.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = detector(gray)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        face = faces[0]\n",
    "        shapes = predictor(gray, face)\n",
    "        shapes = face_utils.shape_to_np(shapes)\n",
    "\n",
    "        eye_img_l, eye_rect_l = crop_eye(gray, eye_points=shapes[36:42])\n",
    "        eye_img_r, eye_rect_r = crop_eye(gray, eye_points=shapes[42:48])\n",
    "\n",
    "        eye_img_l = cv2.resize(eye_img_l, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.resize(eye_img_r, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.flip(eye_img_r, flipCode=1)\n",
    "\n",
    "        eye_input_l = eye_img_l.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "        eye_input_r = eye_img_r.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "\n",
    "        pred_l = model.predict(eye_input_l)\n",
    "        pred_r = model.predict(eye_input_r)\n",
    "\n",
    "        # 눈 깜빡임 주기 확인\n",
    "        if pred_l < 0.2 and pred_r < 0.2:\n",
    "            blink_counter += 1\n",
    "            if blink_counter >= blink_threshold:\n",
    "                speak(\"졸음 운전 감지\")\n",
    "                blink_counter = 0  # 초기화\n",
    "        else:\n",
    "            blink_counter = 0  # 초기화\n",
    "\n",
    "        # visualize\n",
    "        state_l = 'O %.1f' if pred_l > 0.1 else '- %.1f'\n",
    "        state_r = 'O %.1f' if pred_r > 0.1 else '- %.1f'\n",
    "        \n",
    "        state_l = state_l % pred_l\n",
    "        state_r = state_r % pred_r\n",
    "\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_l[0:2]), pt2=tuple(eye_rect_l[2:4]), color=(255, 255, 255), thickness=2)\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_r[0:2]), pt2=tuple(eye_rect_r[2:4]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "        cv2.putText(img, state_l, tuple(eye_rect_l[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img, state_r, tuple(eye_rect_r[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('result', img)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # ESC 키를 누르면 종료\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26, 34, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 34, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 17, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 17, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 8, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               786944    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 880129 (3.36 MB)\n",
      "Trainable params: 880129 (3.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsgu\\AppData\\Local\\Temp\\ipykernel_12740\\1587456468.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  state_l = state_l % pred_l\n",
      "C:\\Users\\alsgu\\AppData\\Local\\Temp\\ipykernel_12740\\1587456468.py:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  state_r = state_r % pred_r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from keras.models import load_model\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "IMG_SIZE = (34, 26)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "model = load_model('models/2024_01_05_14_16_21.h5')\n",
    "model.summary()\n",
    "\n",
    "# Text-to-speech engine 초기화\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)  # 음성 속도 조절\n",
    "\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def crop_eye(img, eye_points):\n",
    "    x1, y1 = np.amin(eye_points, axis=0)\n",
    "    x2, y2 = np.amax(eye_points, axis=0)\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    w = (x2 - x1) * 1.2\n",
    "    h = w * IMG_SIZE[1] / IMG_SIZE[0]\n",
    "\n",
    "    margin_x, margin_y = w / 2, h / 2\n",
    "\n",
    "    min_x, min_y = int(cx - margin_x), int(cy - margin_y)\n",
    "    max_x, max_y = int(cx + margin_x), int(cy + margin_y)\n",
    "\n",
    "    eye_rect = np.rint([min_x, min_y, max_x, max_y]).astype(int)\n",
    "\n",
    "    eye_img = gray[eye_rect[1]:eye_rect[3], eye_rect[0]:eye_rect[2]]\n",
    "\n",
    "    return eye_img, eye_rect\n",
    "\n",
    "capture = cv2.VideoCapture(0)  # 디바이스 아이디\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 2560)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1440)\n",
    "\n",
    "prev_time = time.time()  # 이전에 눈을 감은 시간\n",
    "blink_threshold = 3  # 눈을 깜빡이지 않아도 작동할 시간\n",
    "blink_counter = 0    # 눈을 깜빡인 시간을 측정하는 카운터\n",
    "\n",
    "while True:\n",
    "    ret, img_ori = capture.read()\n",
    "    img = img_ori.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 현재 시간 계산\n",
    "    current_time = time.time()\n",
    "\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        face = faces[0]\n",
    "        shapes = predictor(gray, face)\n",
    "        shapes = face_utils.shape_to_np(shapes)\n",
    "\n",
    "        eye_img_l, eye_rect_l = crop_eye(gray, eye_points=shapes[36:42])\n",
    "        eye_img_r, eye_rect_r = crop_eye(gray, eye_points=shapes[42:48])\n",
    "\n",
    "        eye_img_l = cv2.resize(eye_img_l, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.resize(eye_img_r, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.flip(eye_img_r, flipCode=1)\n",
    "\n",
    "        eye_input_l = eye_img_l.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "        eye_input_r = eye_img_r.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "\n",
    "        pred_l = model.predict(eye_input_l)\n",
    "        pred_r = model.predict(eye_input_r)\n",
    "\n",
    "        # 눈 깜빡임 주기 확인\n",
    "        if pred_l < 0.2 and pred_r < 0.2:\n",
    "            blink_counter += 1\n",
    "            if blink_counter >= blink_threshold:\n",
    "                speak(\"졸음 운전 감지\")\n",
    "                blink_counter = 0  # 초기화\n",
    "        else:\n",
    "            blink_counter = 0  # 초기화\n",
    "\n",
    "        # visualize\n",
    "        state_l = 'O %.1f' if pred_l > 0.1 else '- %.1f'\n",
    "        state_r = 'O %.1f' if pred_r > 0.1 else '- %.1f'\n",
    "        \n",
    "        state_l = state_l % pred_l\n",
    "        state_r = state_r % pred_r\n",
    "\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_l[0:2]), pt2=tuple(eye_rect_l[2:4]), color=(255, 255, 255), thickness=2)\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_r[0:2]), pt2=tuple(eye_rect_r[2:4]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "        cv2.putText(img, state_l, tuple(eye_rect_l[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img, state_r, tuple(eye_rect_r[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # 눈이 감겨 있지 않을 때 현재 시간 갱신\n",
    "        prev_time = current_time\n",
    "    else:\n",
    "        # 눈을 5초 이상 감지하지 못할 때 메시지 출력\n",
    "        elapsed_time = current_time - prev_time\n",
    "        if elapsed_time > 5 and blink_counter == 0:\n",
    "            speak(\"전방 주시 요망\")\n",
    "            blink_counter += 1  # 초기화\n",
    "\n",
    "    cv2.imshow('result', img)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # ESC 키를 누르면 종료\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 눈 사진 캡쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26, 34, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 34, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 17, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 17, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 8, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               786944    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 880129 (3.36 MB)\n",
      "Trainable params: 880129 (3.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsgu\\AppData\\Local\\Temp\\ipykernel_18860\\2308023719.py:83: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  state_l = state_l % pred_l\n",
      "C:\\Users\\alsgu\\AppData\\Local\\Temp\\ipykernel_18860\\2308023719.py:84: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  state_r = state_r % pred_r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "IMG_SIZE = (34, 26)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "model = load_model('models/2023_12_26_11_40_10.h5')\n",
    "model.summary()\n",
    "\n",
    "def crop_eye(img, eye_points):\n",
    "    x1, y1 = np.amin(eye_points, axis=0)\n",
    "    x2, y2 = np.amax(eye_points, axis=0)\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    w = (x2 - x1) * 1.2\n",
    "    h = w * IMG_SIZE[1] / IMG_SIZE[0]\n",
    "\n",
    "    margin_x, margin_y = w / 2, h / 2\n",
    "\n",
    "    min_x, min_y = int(cx - margin_x), int(cy - margin_y)\n",
    "    max_x, max_y = int(cx + margin_x), int(cy + margin_y)\n",
    "\n",
    "    eye_rect = np.rint([min_x, min_y, max_x, max_y]).astype(int)\n",
    "\n",
    "    eye_img = img[eye_rect[1]:eye_rect[3], eye_rect[0]:eye_rect[2]]\n",
    "\n",
    "    return eye_img, eye_rect\n",
    "\n",
    "# main\n",
    "\n",
    "capture = cv2.VideoCapture(0)  # 디바이스 아이디\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "count = 1  # 이미지 파일 저장을 위한 카운터 초기화\n",
    "\n",
    "while cv2.waitKey(10) < 0:  # wait를 늘리면 프레임이 줄어듬\n",
    "  ret, img_ori = capture.read()\n",
    "  img = img_ori.copy()\n",
    "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  faces = detector(gray)\n",
    "  face = faces[0]\n",
    "  shapes = predictor(gray, face)\n",
    "  shapes = face_utils.shape_to_np(shapes)\n",
    "\n",
    "  eye_img_l, eye_rect_l = crop_eye(gray, eye_points=shapes[36:42])\n",
    "  eye_img_r, eye_rect_r = crop_eye(gray, eye_points=shapes[42:48])\n",
    "\n",
    "  eye_img_l = cv2.resize(eye_img_l, dsize=IMG_SIZE)\n",
    "  eye_img_r = cv2.resize(eye_img_r, dsize=IMG_SIZE)\n",
    "  eye_img_r = cv2.flip(eye_img_r, flipCode=1)\n",
    "\n",
    "  # 이미지 저장 경로 및 파일명 설정\n",
    "  \n",
    "  path_l = f'closeEye1/{count}.png'\n",
    "  cv2.imwrite(path_l, eye_img_l)\n",
    "  count += 1  # 카운터 증가\n",
    "  path_r = f'closeEye1/{count}.png'\n",
    "  cv2.imwrite(path_r, eye_img_r)\n",
    "  count += 1  # 카운터 증가\n",
    "  \n",
    "\n",
    "  eye_input_l = eye_img_l.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "  eye_input_r = eye_img_r.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "\n",
    "  pred_l = model.predict(eye_input_l)\n",
    "  pred_r = model.predict(eye_input_r)\n",
    "\n",
    "  # visualize\n",
    "  state_l = 'O %.1f' if pred_l > 0.1 else '- %.1f'\n",
    "  state_r = 'O %.1f' if pred_r > 0.1 else '- %.1f'\n",
    "\n",
    "  state_l = state_l % pred_l\n",
    "  state_r = state_r % pred_r\n",
    "\n",
    "  cv2.rectangle(img, pt1=tuple(eye_rect_l[0:2]), pt2=tuple(eye_rect_l[2:4]), color=(255,255,255), thickness=2)\n",
    "  cv2.rectangle(img, pt1=tuple(eye_rect_r[0:2]), pt2=tuple(eye_rect_r[2:4]), color=(255,255,255), thickness=2)\n",
    "\n",
    "  cv2.putText(img, state_l, tuple(eye_rect_l[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "  cv2.putText(img, state_r, tuple(eye_rect_r[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "  cv2.imshow('result', img)\n",
    "  capture.release()\n",
    "  cv2.waitKey()\n",
    "  cv2.destroyAllWindows()\n",
    "  cv2.waitKey(1)\n",
    "\n",
    "# capture.release()\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()\n",
    "# cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26, 34, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 34, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 17, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 17, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 8, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               786944    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 880129 (3.36 MB)\n",
      "Trainable params: 880129 (3.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsgu\\AppData\\Local\\Temp\\ipykernel_12740\\3024266155.py:82: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  state_l = state_l % pred_l\n",
      "C:\\Users\\alsgu\\AppData\\Local\\Temp\\ipykernel_12740\\3024266155.py:83: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  state_r = state_r % pred_r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "IMG_SIZE = (34, 26)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "model = load_model('models/2023_12_26_11_40_10.h5')\n",
    "model.summary()\n",
    "\n",
    "def crop_eye(img, eye_points):\n",
    "    x1, y1 = np.amin(eye_points, axis=0)\n",
    "    x2, y2 = np.amax(eye_points, axis=0)\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    w = (x2 - x1) * 1.2\n",
    "    h = w * IMG_SIZE[1] / IMG_SIZE[0]\n",
    "\n",
    "    margin_x, margin_y = w / 2, h / 2\n",
    "\n",
    "    min_x, min_y = int(cx - margin_x), int(cy - margin_y)\n",
    "    max_x, max_y = int(cx + margin_x), int(cy + margin_y)\n",
    "\n",
    "    eye_rect = np.rint([min_x, min_y, max_x, max_y]).astype(int)\n",
    "\n",
    "    eye_img = img[eye_rect[1]:eye_rect[3], eye_rect[0]:eye_rect[2]]\n",
    "\n",
    "    return eye_img, eye_rect\n",
    "\n",
    "# main\n",
    "\n",
    "capture = cv2.VideoCapture(0)  # 디바이스 아이디\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "count = 341  # 이미지 파일 저장을 위한 카운터 초기화\n",
    "\n",
    "while True:\n",
    "    ret, img_ori = capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = img_ori.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if faces:\n",
    "        face = faces[0]\n",
    "        shapes = predictor(gray, face)\n",
    "        shapes = face_utils.shape_to_np(shapes)\n",
    "\n",
    "        eye_img_l, eye_rect_l = crop_eye(gray, eye_points=shapes[36:42])\n",
    "        eye_img_r, eye_rect_r = crop_eye(gray, eye_points=shapes[42:48])\n",
    "\n",
    "        eye_img_l = cv2.resize(eye_img_l, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.resize(eye_img_r, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.flip(eye_img_r, flipCode=1)\n",
    "\n",
    "        # 이미지 저장 경로 및 파일명 설정\n",
    "        path_l = f'openEye1/{count}.png'\n",
    "        cv2.imwrite(path_l, eye_img_l)\n",
    "        count += 1  # 카운터 증가\n",
    "        path_r = f'openEye1/{count}.png'\n",
    "        cv2.imwrite(path_r, eye_img_r)\n",
    "        count += 1  # 카운터 증가\n",
    "\n",
    "        eye_input_l = eye_img_l.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "        eye_input_r = eye_img_r.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "\n",
    "        pred_l = model.predict(eye_input_l)\n",
    "        pred_r = model.predict(eye_input_r)\n",
    "\n",
    "        # visualize\n",
    "        state_l = 'O %.1f' if pred_l > 0.1 else '- %.1f'\n",
    "        state_r = 'O %.1f' if pred_r > 0.1 else '- %.1f'\n",
    "\n",
    "        state_l = state_l % pred_l\n",
    "        state_r = state_r % pred_r\n",
    "\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_l[0:2]), pt2=tuple(eye_rect_l[2:4]), color=(255,255,255), thickness=2)\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_r[0:2]), pt2=tuple(eye_rect_r[2:4]), color=(255,255,255), thickness=2)\n",
    "\n",
    "        cv2.putText(img, state_l, tuple(eye_rect_l[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "        cv2.putText(img, state_r, tuple(eye_rect_r[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "    \n",
    "    cv2.imshow('result', img)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # ESC 키를 누르면 종료\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "                                              0.0/9.2 MB ? eta -:--:--\n",
      "     -----                                    1.3/9.2 MB 40.3 MB/s eta 0:00:01\n",
      "     ----------                               2.5/9.2 MB 26.5 MB/s eta 0:00:01\n",
      "     ---------------                          3.7/9.2 MB 29.2 MB/s eta 0:00:01\n",
      "     --------------------                     4.8/9.2 MB 25.4 MB/s eta 0:00:01\n",
      "     -------------------------                5.9/9.2 MB 25.0 MB/s eta 0:00:01\n",
      "     ------------------------------           7.1/9.2 MB 25.1 MB/s eta 0:00:01\n",
      "     -------------------------------          7.3/9.2 MB 26.1 MB/s eta 0:00:01\n",
      "     ------------------------------------     8.3/9.2 MB 22.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  9.2/9.2 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  9.2/9.2 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 9.2/9.2 MB 19.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\alsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.11.4-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "                                              0.0/44.1 MB ? eta -:--:--\n",
      "     -                                        1.1/44.1 MB 23.3 MB/s eta 0:00:02\n",
      "     --                                       2.6/44.1 MB 27.9 MB/s eta 0:00:02\n",
      "     ---                                      4.0/44.1 MB 32.0 MB/s eta 0:00:02\n",
      "     ----                                     5.4/44.1 MB 31.1 MB/s eta 0:00:02\n",
      "     -----                                    6.5/44.1 MB 29.6 MB/s eta 0:00:02\n",
      "     -------                                  7.9/44.1 MB 29.5 MB/s eta 0:00:02\n",
      "     --------                                 9.0/44.1 MB 28.7 MB/s eta 0:00:02\n",
      "     ---------                               10.4/44.1 MB 28.4 MB/s eta 0:00:02\n",
      "     ----------                              12.3/44.1 MB 31.2 MB/s eta 0:00:02\n",
      "     ------------                            13.9/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------                          16.1/44.1 MB 32.8 MB/s eta 0:00:01\n",
      "     ---------------                         17.5/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     -----------------                       19.4/44.1 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------                      21.1/44.1 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------                    22.7/44.1 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------                   24.5/44.1 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------------                 26.3/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------                28.2/44.1 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------              29.5/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------             31.1/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------            32.8/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------          34.4/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------------         36.0/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------------------------       37.9/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     -----------------------------------     39.7/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    41.1/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   42.8/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.1/44.1 MB 25.2 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "                                              0.0/302.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 302.2/302.2 kB 19.5 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.11.4 threadpoolctl-3.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_data [[[108  96  90 ...  35  37  37]\n",
      "  [ 99  87  89 ...  28  28  32]\n",
      "  [100  94  97 ...  70  63  53]\n",
      "  ...\n",
      "  [119 119 119 ... 107 105 112]\n",
      "  [122 120 119 ... 113 109 113]\n",
      "  [119 118 117 ... 116 114 114]]\n",
      "\n",
      " [[ 88  87  89 ...  34  31  36]\n",
      "  [ 94  92  91 ...  46  31  31]\n",
      "  [ 93  93  93 ...  90  74  57]\n",
      "  ...\n",
      "  [104 104 102 ... 107 105 103]\n",
      "  [106 105 105 ... 108 105 106]\n",
      "  [106 105 106 ... 107 106 108]]\n",
      "\n",
      " [[ 98  88  89 ...  44  44  48]\n",
      "  [ 89  85  90 ...  78  62  45]\n",
      "  [ 97  97  95 ... 106 111 109]\n",
      "  ...\n",
      "  [123 121 119 ... 122 119 118]\n",
      "  [122 121 121 ... 122 120 120]\n",
      "  [124 121 122 ... 123 120 120]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[171 175 176 ... 138 131 130]\n",
      "  [176 175 175 ... 137 135 129]\n",
      "  [174 171 175 ... 130 131 129]\n",
      "  ...\n",
      "  [199 199 198 ... 160 166 162]\n",
      "  [199 197 195 ... 162 161 162]\n",
      "  [201 198 199 ... 164 163 166]]\n",
      "\n",
      " [[179 177 182 ... 128 128 128]\n",
      "  [183 180 186 ... 126 126 127]\n",
      "  [182 185 185 ... 127 129 130]\n",
      "  ...\n",
      "  [215 212 211 ... 194 193 192]\n",
      "  [218 215 214 ... 190 186 194]\n",
      "  [221 215 214 ... 184 183 184]]\n",
      "\n",
      " [[166 171 170 ... 133 131 131]\n",
      "  [169 174 175 ... 132 131 132]\n",
      "  [173 174 174 ... 128 129 130]\n",
      "  ...\n",
      "  [200 201 198 ... 162 162 162]\n",
      "  [200 202 199 ... 161 163 164]\n",
      "  [203 200 198 ... 166 167 172]]]\n",
      "data       state                                              image\n",
      "0      open  [[108, 96, 90, 89, 90, 90, 96, 77, 76, 82, 71,...\n",
      "1      open  [[88, 87, 89, 86, 89, 86, 83, 81, 81, 78, 79, ...\n",
      "2      open  [[98, 88, 89, 90, 91, 91, 89, 88, 88, 94, 90, ...\n",
      "3      open  [[149, 150, 145, 147, 147, 145, 147, 147, 146,...\n",
      "4      open  [[145, 141, 146, 141, 145, 146, 145, 144, 145,...\n",
      "...     ...                                                ...\n",
      "2308  close  [[163, 168, 168, 167, 169, 172, 175, 173, 174,...\n",
      "2309  close  [[187, 188, 185, 186, 188, 188, 187, 188, 183,...\n",
      "2310  close  [[171, 175, 176, 180, 175, 177, 175, 174, 176,...\n",
      "2311  close  [[179, 177, 182, 181, 181, 183, 186, 178, 184,...\n",
      "2312  close  [[166, 171, 170, 174, 175, 178, 176, 177, 177,...\n",
      "\n",
      "[2313 rows x 2 columns]\n",
      "(1850, 26, 34) (1850, 1)\n",
      "(463, 26, 34) (463, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 이미지를 읽고 지정된 크기로 조정하는 함수\n",
    "def read_and_resize(file_path, target_size=(34, 26)):\n",
    "    img = Image.open(file_path)\n",
    "    img = img.resize(target_size, resample=Image.BICUBIC)\n",
    "    return np.array(img)\n",
    "\n",
    "# 각각의 이미지 파일이 저장된 폴더 경로\n",
    "open_folder_path = \"./openEye1/\"\n",
    "close_folder_path = \"./closeEye1/\"\n",
    "\n",
    "# 각 폴더에 있는 이미지 파일들의 경로를 리스트로 수집\n",
    "open_image_files = [os.path.join(open_folder_path, f) for f in os.listdir(open_folder_path) if f.endswith(\".png\")]\n",
    "close_image_files = [os.path.join(close_folder_path, f) for f in os.listdir(close_folder_path) if f.endswith(\".png\")]\n",
    "\n",
    "# 이미지 데이터를 읽어서 리스트에 추가\n",
    "image_data = []\n",
    "state = []\n",
    "\n",
    "# 눈 뜬 사진 추가\n",
    "for open_file in open_image_files:\n",
    "    open_data = read_and_resize(open_file)\n",
    "    image_data.append(open_data)\n",
    "    state.append(\"open\")\n",
    "\n",
    "# 눈 감은 사진 추가\n",
    "for close_file in close_image_files:\n",
    "    close_data = read_and_resize(close_file)\n",
    "    image_data.append(close_data)\n",
    "    state.append(\"close\")\n",
    "\n",
    "# 데이터셋 생성\n",
    "data = pd.DataFrame({\"state\": state, \"image\": image_data})\n",
    "\n",
    "# 데이터를 랜덤하게 섞은 후 훈련 데이터와 검증 데이터로 분할\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "print(\"image_data\", np.array(image_data))\n",
    "print(\"data\", data)\n",
    "data.to_csv(\"newDataset/dataset.csv\", index=False)\n",
    "train_data.to_csv(\"newDataset/train_dataset.csv\")\n",
    "val_data.to_csv(\"newDataset/val_dataset.csv\")\n",
    "\n",
    "# 이미지 데이터를 NumPy 배열로 변환\n",
    "x_train = np.array(train_data[\"image\"].tolist()).astype(np.float32)\n",
    "x_val = np.array(val_data[\"image\"].tolist()).astype(np.float32)\n",
    "\n",
    "# 레이블을 NumPy 배열로 변환\n",
    "y_train = np.array(train_data[\"state\"].map({\"open\": 1, \"close\": 0})).reshape(-1, 1).astype(np.float32)\n",
    "y_val = np.array(val_data[\"state\"].map({\"open\": 1, \"close\": 0})).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# NumPy 배열을 파일로 저장\n",
    "np.save(\"newDataset/x_train.npy\", x_train)\n",
    "np.save(\"newDataset/x_val.npy\", x_val)\n",
    "np.save(\"newDataset/y_train.npy\", y_train)\n",
    "np.save(\"newDataset/y_val.npy\", y_val)\n",
    "\n",
    "# 출력 확인\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제스쳐 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_display\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DisplayHand\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_mediapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MediaPipeHand\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_joint_angle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GestureRecognition\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alsgu\\python\\eye_blink_detector\\utils_mediapipe.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define default camera intrinsic\u001b[39;00m\n\u001b[0;32m     13\u001b[0m img_width  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m640\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\mediapipe\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[1;32mc:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[1;32mc:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classifier\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_embedder\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m AudioClassifier \u001b[38;5;241m=\u001b[39m \u001b[43maudio_classifier\u001b[49m\u001b[38;5;241m.\u001b[39mAudioClassifier\n\u001b[0;32m     22\u001b[0m AudioClassifierOptions \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifierOptions\n\u001b[0;32m     23\u001b[0m AudioClassifierResult \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifierResult\n",
      "\u001b[1;31mNameError\u001b[0m: name 'audio_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from utils_display import DisplayHand\n",
    "from utils_mediapipe import MediaPipeHand\n",
    "from utils_joint_angle import GestureRecognition\n",
    "import argparse\n",
    "\n",
    "# ArgumentParser 생성\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-m', '--mode', default='eval', help='train / eval')\n",
    "args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "# mode 변수 설정\n",
    "mode = args.mode\n",
    "\n",
    "# mediapipe hand 클래스 로드\n",
    "pipe = MediaPipeHand(static_image_mode=False, max_num_hands=2)\n",
    "\n",
    "# 디스플레이 클래스 로드\n",
    "disp = DisplayHand(max_num_hands=2)\n",
    "\n",
    "# 비디오 캡처 시작\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 2560)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1440)\n",
    "\n",
    "# 제스처 인식 클래스 로드\n",
    "gest = GestureRecognition(mode)\n",
    "\n",
    "counter = 0\n",
    "class_label = 0\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 3인칭 뷰를 위해 이미지 뒤집기\n",
    "    img = cv2.flip(img, 1)\n",
    "\n",
    "    # 참조로 전달하기 위해 이미지를 쓸 수 없게 표시(옵션)\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # 키포인트를 추출하기 위해 피드포워드\n",
    "    param = pipe.forward(img)\n",
    "    if (param[0]['class'] is not None) and (mode=='eval'):\n",
    "        param[0]['gesture'] = gest.eval(param[0]['angle'])\n",
    "\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # 키포인트 표시\n",
    "    cv2.imshow('img 2D', disp.draw2d(img.copy(), param))\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key==27:\n",
    "        break\n",
    "    if key==97 and (param[0]['class'] is not None) and (mode=='train'):\n",
    "        print('train mode')\n",
    "        class_label = (class_label + 1) % 11\n",
    "        print('제스처로 변경:', list(gest.gesture)[class_label])\n",
    "    if key==32 and (param[0]['class'] is not None) and (mode=='train'):\n",
    "        # 스페이스바를 눌러 훈련 데이터 기록\n",
    "        gest.train(param[0]['angle'], class_label)\n",
    "        print('저장됨:', list(gest.gesture)[class_label], '카운터:', counter) # 각 클래스에 대해 약 10번 기록\n",
    "        counter += 1\n",
    "    if key==32 and (param[0]['class'] is not None) and (mode=='eval'):\n",
    "        cv2.waitKey(0) # 사용자가 아무 키나 누를 때까지 디스플레이 일시 중지        \n",
    "\n",
    "pipe.pipe.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (0.10.9)\n",
      "Requirement already satisfied: absl-py in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from mediapipe) (2.0.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from mediapipe) (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from mediapipe) (1.24.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alsgu\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alsgu\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alsgu\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade mediapipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제스처 음성 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "WARNING:tensorflow:From c:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from utils_display import DisplayHand\n",
    "from utils_mediapipe import MediaPipeHand\n",
    "from utils_joint_angle import GestureRecognition\n",
    "import numpy as np\n",
    "import time\n",
    "import pyttsx3\n",
    "import argparse\n",
    "\n",
    "# ArgumentParser 생성\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-m', '--mode', default='eval', help='train / eval')\n",
    "args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "# mode 변수 설정\n",
    "mode = args.mode\n",
    "\n",
    "# mediapipe hand 클래스 로드\n",
    "pipe = MediaPipeHand(static_image_mode=False, max_num_hands=1)\n",
    "\n",
    "# 디스플레이 클래스 로드\n",
    "disp = DisplayHand(max_num_hands=1)\n",
    "\n",
    "# 비디오 캡처 시작\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 제스처 인식 클래스 로드\n",
    "gest = GestureRecognition(mode)\n",
    "\n",
    "# 음성 출력 엔진 초기화\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# 'home' 제스처가 인식된 시간을 저장하는 변수\n",
    "home_start_time = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 3인칭 뷰를 위해 이미지 뒤집기\n",
    "    img = cv2.flip(img, 1)\n",
    "\n",
    "    # 참조로 전달하기 위해 이미지를 쓸 수 없게 표시(옵션)\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # 키포인트를 추출하기 위해 피드포워드\n",
    "    param = pipe.forward(img)\n",
    "    \n",
    "    if (param[0]['class'] is not None) and (mode == 'eval'):\n",
    "        param[0]['gesture'] = gest.eval(param[0]['angle'])\n",
    "        \n",
    "        # 'home' 제스처가 인식되면 음성 출력 및 시간 기록\n",
    "        if param[0]['gesture'] == 'home':\n",
    "            if home_start_time is None:\n",
    "                home_start_time = time.time()\n",
    "            else:\n",
    "                elapsed_time = time.time() - home_start_time\n",
    "                if elapsed_time >= 3:  # 3초 이상 동안 'home' 제스처가 인식되면\n",
    "                    engine.say('안내를 시작합니다')\n",
    "                    engine.runAndWait()\n",
    "                    home_start_time = None  # 초기화\n",
    "        else:\n",
    "            home_start_time = None  # 'home' 제스처가 아닌 경우 초기화\n",
    "\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # 키포인트 표시\n",
    "    cv2.imshow('img 2D', disp.draw2d(img.copy(), param))\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "engine.stop()\n",
    "pipe.pipe.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_display\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DisplayHand\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_mediapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MediaPipeHand\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_joint_angle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GestureRecognition\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alsgu\\python\\eye_blink_detector\\utils_mediapipe.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define default camera intrinsic\u001b[39;00m\n\u001b[0;32m     13\u001b[0m img_width  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m640\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\mediapipe\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[1;32mc:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[1;32mc:\\Users\\alsgu\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classifier\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_embedder\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m AudioClassifier \u001b[38;5;241m=\u001b[39m \u001b[43maudio_classifier\u001b[49m\u001b[38;5;241m.\u001b[39mAudioClassifier\n\u001b[0;32m     22\u001b[0m AudioClassifierOptions \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifierOptions\n\u001b[0;32m     23\u001b[0m AudioClassifierResult \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifierResult\n",
      "\u001b[1;31mNameError\u001b[0m: name 'audio_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from utils_display import DisplayHand\n",
    "from utils_mediapipe import MediaPipeHand\n",
    "from utils_joint_angle import GestureRecognition\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from tensorflow.keras.models import load_model\n",
    "import pyttsx3\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "IMG_SIZE = (34, 26)\n",
    "\n",
    "# ArgumentParser 생성\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-m', '--mode', default='eval', help='train / eval')\n",
    "args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "# mode 변수 설정\n",
    "mode = args.mode\n",
    "\n",
    "# mediapipe hand 클래스 로드\n",
    "pipe = MediaPipeHand(static_image_mode=False, max_num_hands=1)\n",
    "\n",
    "# 디스플레이 클래스 로드\n",
    "disp = DisplayHand(max_num_hands=1)\n",
    "\n",
    "# 비디오 캡처 시작\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 2560)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1440)\n",
    "\n",
    "# 제스처 인식 클래스 로드\n",
    "gest = GestureRecognition(mode)\n",
    "\n",
    "# 음성 출력 엔진 초기화\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)  # 음성 속도 조절\n",
    "\n",
    "# 'home' 제스처가 인식된 시간을 저장하는 변수\n",
    "home_start_time = None\n",
    "\n",
    "# dlib face detector 및 shape predictor 초기화\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# 눈 깜빡임 주기 확인을 위한 변수들\n",
    "blink_threshold = 3  # 눈을 깜빡이지 않아도 작동할 시간\n",
    "blink_counter = 0    # 눈을 깜빡인 시간을 측정하는 카운터\n",
    "\n",
    "# 딥러닝 모델 로드\n",
    "model = load_model('models/2024_01_05_14_16_21.h5')\n",
    "model.summary()\n",
    "\n",
    "# 이전에 눈을 감은 시간\n",
    "prev_time = time.time()\n",
    "\n",
    "def crop_eye(img, eye_points):\n",
    "    x1, y1 = np.amin(eye_points, axis=0)\n",
    "    x2, y2 = np.amax(eye_points, axis=0)\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    w = (x2 - x1) * 1.2\n",
    "    h = w * IMG_SIZE[1] / IMG_SIZE[0]\n",
    "\n",
    "    margin_x, margin_y = w / 2, h / 2\n",
    "\n",
    "    min_x, min_y = int(cx - margin_x), int(cy - margin_y)\n",
    "    max_x, max_y = int(cx + margin_x), int(cy + margin_y)\n",
    "\n",
    "    eye_rect = np.rint([min_x, min_y, max_x, max_y]).astype(int)\n",
    "\n",
    "    eye_img = img[eye_rect[1]:eye_rect[3], eye_rect[0]:eye_rect[2]]\n",
    "\n",
    "    return eye_img, eye_rect\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img_ori = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = img_ori.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 3인칭 뷰를 위해 이미지 뒤집기\n",
    "    #img = cv2.flip(img, 1)\n",
    "\n",
    "    # 참조로 전달하기 위해 이미지를 쓸 수 없게 표시(옵션)\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # 키포인트를 추출하기 위해 피드포워드\n",
    "    param = pipe.forward(img)\n",
    "\n",
    "    if (param[0]['class'] is not None) and (mode == 'eval'):\n",
    "        param[0]['gesture'] = gest.eval(param[0]['angle'])\n",
    "\n",
    "        # 'home' 제스처가 인식되면 음성 출력 및 시간 기록\n",
    "        if param[0]['gesture'] == 'home':\n",
    "            if home_start_time is None:\n",
    "                home_start_time = time.time()\n",
    "            else:\n",
    "                elapsed_time = time.time() - home_start_time\n",
    "                if elapsed_time >= 3:  # 3초 이상 동안 'home' 제스처가 인식되면\n",
    "                    engine.say('안내를 시작합니다')\n",
    "                    engine.runAndWait()\n",
    "                    home_start_time = None  # 초기화\n",
    "        else:\n",
    "            home_start_time = None  # 'home' 제스처가 아닌 경우 초기화\n",
    "\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # 키포인트 표시\n",
    "    img_2d = disp.draw2d(img.copy(), param)\n",
    "\n",
    "    # dlib 얼굴 및 랜드마크 검출\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        face = faces[0]\n",
    "        shapes = predictor(gray, face)\n",
    "        shapes = face_utils.shape_to_np(shapes)\n",
    "\n",
    "        eye_img_l, eye_rect_l = crop_eye(gray, eye_points=shapes[36:42])\n",
    "        eye_img_r, eye_rect_r = crop_eye(gray, eye_points=shapes[42:48])\n",
    "\n",
    "        eye_img_l = cv2.resize(eye_img_l, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.resize(eye_img_r, dsize=IMG_SIZE)\n",
    "\n",
    "        # 눈 이미지 수평으로 뒤집기\n",
    "        eye_img_l_flipped = cv2.flip(eye_img_l, 1)\n",
    "        eye_img_r_flipped = cv2.flip(eye_img_r, 1)\n",
    "\n",
    "        eye_input_l = eye_img_l.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "        eye_input_r = eye_img_r.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "\n",
    "        pred_l = model.predict(eye_input_l)\n",
    "        pred_r = model.predict(eye_input_r)\n",
    "\n",
    "        # 눈 깜빡임 주기 확인\n",
    "        if pred_l < 0.2 and pred_r < 0.2:\n",
    "            blink_counter += 1\n",
    "            if blink_counter >= blink_threshold:\n",
    "                engine.say(\"졸음 운전 감지\")\n",
    "                engine.runAndWait()\n",
    "                blink_counter = 0  # 초기화\n",
    "        else:\n",
    "            blink_counter = 0  # 초기화\n",
    "\n",
    "        # visualize\n",
    "        state_l = 'O %.1f' if pred_l > 0.1 else '- %.1f'\n",
    "        state_r = 'O %.1f' if pred_r > 0.1 else '- %.1f'\n",
    "\n",
    "        state_l = state_l % pred_l\n",
    "        state_r = state_r % pred_r\n",
    "\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_l[0:2]), pt2=tuple(eye_rect_l[2:4]), color=(255, 255, 255), thickness=2)\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_r[0:2]), pt2=tuple(eye_rect_r[2:4]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "        cv2.putText(img, state_l, tuple(eye_rect_l[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img, state_r, tuple(eye_rect_r[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    # 이미지 합치기\n",
    "    combined_img = cv2.addWeighted(img_2d, 0.5, img, 0.5, 0)\n",
    "\n",
    "    cv2.imshow('result', combined_img)  # 두 결과를 합쳐서 표시하는 윈도우\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "engine.stop()\n",
    "pipe.pipe.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지도 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/dc/72/96b5afa16908f9abc7c24b70adfd3a46c9740eb728ddfeab28379e38eaf9/selenium-4.16.0-py3-none-any.whl.metadata\n",
      "  Downloading selenium-4.16.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/14/fb/9299cf74953f473a15accfdbe2c15218e766bae8c796f2567c83bae03e98/trio-0.24.0-py3-none-any.whl.metadata\n",
      "  Downloading trio-0.24.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\alsgu\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.0 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.2/10.0 MB 12.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.2/10.0 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.5/10.0 MB 22.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.1/10.0 MB 23.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 27.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/10.0 MB 28.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 29.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 26.5 MB/s eta 0:00:00\n",
      "Downloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
      "   ---------------------------------------- 0.0/460.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 460.2/460.2 kB 28.1 MB/s eta 0:00:00\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.16.0 sniffio-1.3.0 trio-0.24.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 거리: 46.32 km\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import folium\n",
    "import json\n",
    "import os\n",
    "import webbrowser\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 중간 지점 계산 함수\n",
    "def calculate_midpoint(lat1, lon1, lat2, lon2):\n",
    "    return ((lat1 + lat2) / 2, (lon1 + lon2) / 2)\n",
    "\n",
    "# https://ipstack.com\n",
    "##########ip주소로 위도, 경도 찾는 API##########\n",
    "key = 'c887f69dd63ddb1cd9d4f53408974e46'\n",
    "send_url = 'http://api.ipstack.com/check?access_key=' + key\n",
    "\n",
    "# tmap app key\n",
    "APP_KEY = \"HpVe5crDV38GvHu7tUyMA5CSmhca5PQk3vRWkFH0\"\n",
    "\n",
    "headers = {\n",
    "    \"appKey\": APP_KEY\n",
    "}\n",
    "# ip 주소 위치에 따른 위도 경도\n",
    "r = requests.get(send_url)\n",
    "j = json.loads(r.text)\n",
    "\n",
    "lat = j['latitude'] # 위도\n",
    "lon = j['longitude'] # 경도\n",
    "\n",
    "start = str(lat) + \", \" + str(lon)\n",
    "\n",
    "# 도착지의 위도와 경도\n",
    "end_lat =  37.28406376153335\n",
    "end_lon = 126.85166240118258\n",
    "\n",
    "end = str(end_lat) + \", \" + str(end_lon)\n",
    "\n",
    "# 중간 지점 계산\n",
    "midpoint_lat, midpoint_lon = calculate_midpoint(lat, lon, end_lat, end_lon)\n",
    "\n",
    "\n",
    "# 티맵 경로 안내 API URL\n",
    "url = f\"https://api2.sktelecom.com/tmap/routes?version=1&format=json&startX={start.split(',')[1]}&startY={start.split(',')[0]}&endX={end.split(',')[1]}&endY={end.split(',')[0]}\"\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)       \n",
    "\n",
    "# 응답이 성공적인지 확인합니다.\n",
    "if response.status_code == 200:\n",
    "    # API 응답에서 경로 정보를 추출합니다.\n",
    "    routes_info = response.json()[\"features\"]\n",
    "\n",
    "    # 경로를 그릴 좌표들의 리스트를 생성합니다.\n",
    "    path_coordinates = []\n",
    "    for feature in routes_info:\n",
    "        if feature[\"geometry\"][\"type\"] == \"LineString\":\n",
    "            # 좌표계가 [경도, 위도] 형태로 되어 있으므로, [위도, 경도] 순으로 바꿔서 저장합니다.\n",
    "            path_coordinates.extend([[coord[1], coord[0]] for coord in feature[\"geometry\"][\"coordinates\"]])\n",
    "\n",
    "    # 예상 이동 시간을 추출합니다.\n",
    "    total_time = sum([feature[\"properties\"][\"totalTime\"] for feature in routes_info if \"totalTime\" in feature[\"properties\"]])\n",
    "\n",
    "    # 현재 시각을 구합니다.\n",
    "    now = datetime.now()\n",
    "    \n",
    "    # 현재 시각에 total_time초를  더합니다.\n",
    "    future_time = now + timedelta(seconds=total_time)\n",
    "    \n",
    "    # 도착 시간 저장\n",
    "    arrivalTime =  f\"예상 도착 시간:{future_time.hour}시 {future_time.minute}분\"\n",
    "    \n",
    "    # 초 단위로 얻은 소요 시간을 시, 분, 초로 나누어 저장\n",
    "    if total_time > 60:\n",
    "        min = total_time // 60\n",
    "        sec = total_time % 60\n",
    "        if min > 60:\n",
    "            hour = min // 60\n",
    "            min = min % 60\n",
    "            total_time = str(hour) + \"시간 \" + str(min) + \"분 \" + str(sec) + \"초\"\n",
    "        else:\n",
    "            total_time = str(min) + \"분 \" + str(sec) + \"초\"\n",
    "    else:\n",
    "        total_time = str(total_time) + \"초\"\n",
    "    requireTime = f\"예상 이동 시간: {total_time}\"\n",
    "    \n",
    "    #마커 생성\n",
    "    start_marker = folium.Marker(location=path_coordinates[0], popup=folium.Popup('<strong>현재위치</strong><br>'+requireTime, max_width=300, show=True), icon=folium.Icon(color='green'))\n",
    "    end_marker = folium.Marker(location=path_coordinates[-1], popup=folium.Popup('<strong>집</strong><br>'+arrivalTime, max_width=300, show=True), icon=folium.Icon(color='red'))\n",
    "\n",
    "    #km 계산\n",
    "    total_distance = sum([feature[\"properties\"][\"totalDistance\"] for feature in routes_info if \"totalDistance\" in feature[\"properties\"]])\n",
    "    total_distance_km = total_distance / 1000  # 미터를 킬로미터로 변환\n",
    "\n",
    "    if 0 <= total_distance_km <= 15:\n",
    "        zoom_level = 13\n",
    "    elif 15.1 <= total_distance_km <= 30:\n",
    "        zoom_level = 12\n",
    "    elif 30.1 <= total_distance_km <= 110:\n",
    "        zoom_level = 11\n",
    "    elif 110.1 <= total_distance_km <= 230:\n",
    "        zoom_level = 10\n",
    "    elif 230.1 <= total_distance_km <= 400:\n",
    "        zoom_level = 9\n",
    "    else:\n",
    "        zoom_level = 8\n",
    "    \n",
    "    # Folium 지도 객체를 생성합니다.\n",
    "    # 출발지의 위도와 경도를 중심으로 지도를 생성합니다.\n",
    "    map = folium.Map(location=[midpoint_lat, midpoint_lon], zoom_start=zoom_level)\n",
    "\n",
    "    # 출발지와 도착지 마커를 지도에 추가합니다.\n",
    "    start_marker.add_to(map)\n",
    "    end_marker.add_to(map)\n",
    "    \n",
    "    # Folium의 PolyLine을 사용하여 지도에 선을 그립니다.\n",
    "    folium.PolyLine(path_coordinates, color=\"red\", weight=3, opacity=1).add_to(map)\n",
    "\n",
    "    # 지도를 HTML 파일로 저장합니다.\n",
    "    map.save(\"route.html\")\n",
    "    \n",
    "    # 총 거리 추출 및 km 단위로 변환\n",
    "    print(f\"총 거리: {total_distance_km:.2f} km\")\n",
    "    \n",
    "    #webbrowser.open(\"route.html\")\n",
    "    \n",
    "    # 크롬 드라이버를 설정합니다.\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    #브라우저 창 크기 설정\n",
    "    driver.set_window_size(1920, 1080)\n",
    "    \n",
    "    # HTML 파일의 경로를 지정합니다. 파일이 위치한 경로를 정확하게 입력해야 합니다.\n",
    "    file_path = 'file://' + os.path.realpath('route.html')\n",
    "\n",
    "    # 크롬에서 HTML 파일을 엽니다.\n",
    "    driver.get(file_path)\n",
    "\n",
    "    # 5초간 대기합니다.\n",
    "    time.sleep(10)\n",
    "\n",
    "    # 현재 탭을 닫습니다.\n",
    "    driver.close()\n",
    "    \n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 찐최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26, 34, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 34, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 17, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 17, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 8, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               786944    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 880129 (3.36 MB)\n",
      "Trainable params: 880129 (3.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from utils_display import DisplayHand\n",
    "from utils_mediapipe import MediaPipeHand\n",
    "from utils_joint_angle import GestureRecognition\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from tensorflow.keras.models import load_model\n",
    "import pyttsx3\n",
    "import time\n",
    "import argparse\n",
    "from tmap import route\n",
    "\n",
    "def crop_eye(img, eye_points):\n",
    "    x1, y1 = np.amin(eye_points, axis=0)\n",
    "    x2, y2 = np.amax(eye_points, axis=0)\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    w = (x2 - x1) * 1.2\n",
    "    h = w * IMG_SIZE[1] / IMG_SIZE[0]\n",
    "\n",
    "    margin_x, margin_y = w / 2, h / 2\n",
    "\n",
    "    min_x, min_y = int(cx - margin_x), int(cy - margin_y)\n",
    "    max_x, max_y = int(cx + margin_x), int(cy + margin_y)\n",
    "\n",
    "    eye_rect = np.rint([min_x, min_y, max_x, max_y]).astype(int)\n",
    "\n",
    "    eye_img = img[eye_rect[1]:eye_rect[3], eye_rect[0]:eye_rect[2]]\n",
    "\n",
    "    return eye_img, eye_rect\n",
    "\n",
    "\n",
    "# 출발지 도착지 위,경도 중간 지점 계산 함수\n",
    "def calculate_midpoint(lat1, lon1, lat2, lon2):\n",
    "    return ((lat1 + lat2) / 2, (lon1 + lon2) / 2)\n",
    "\n",
    "IMG_SIZE = (34, 26)\n",
    "\n",
    "# ArgumentParser 생성\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-m', '--mode', default='eval', help='train / eval')\n",
    "args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "# mode 변수 설정\n",
    "mode = args.mode\n",
    "\n",
    "# mediapipe hand 클래스 로드\n",
    "pipe = MediaPipeHand(static_image_mode=False, max_num_hands=1)\n",
    "\n",
    "# 딥러닝 모델 로드\n",
    "model = load_model('models/2024_01_05_14_16_21.h5')\n",
    "model.summary()\n",
    "\n",
    "# 디스플레이 클래스 로드\n",
    "disp = DisplayHand(max_num_hands=1)\n",
    "\n",
    "# dlib face detector 및 shape predictor 초기화\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# 비디오 캡처 시작\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 2560)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1440)\n",
    "\n",
    "# 제스처 인식 클래스 로드\n",
    "gest = GestureRecognition(mode)\n",
    "\n",
    "# 음성 출력 엔진 초기화\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)  # 음성 속도 조절\n",
    "\n",
    "# 'home' 제스처가 인식된 시간을 저장하는 변수\n",
    "home_start_time = None\n",
    "\n",
    "# 눈 깜빡임 주기 확인을 위한 변수들\n",
    "blink_threshold = 3  # 눈을 깜빡이지 않아도 작동할 시간\n",
    "blink_counter = 0    # 눈을 깜빡인 시간을 측정하는 카운터\n",
    "\n",
    "# 이전에 눈을 감은 시간\n",
    "prev_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img_ori = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = img_ori.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 참조로 전달하기 위해 이미지를 쓸 수 없게 표시\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # 키포인트를 추출하기 위해 피드포워드\n",
    "    param = pipe.forward(img)\n",
    "\n",
    "    if (param[0]['class'] is not None) and (mode == 'eval'):\n",
    "        param[0]['gesture'] = gest.eval(param[0]['angle'])\n",
    "\n",
    "        # 'home' 제스처가 인식되면 음성 출력 및 시간 기록\n",
    "        if param[0]['gesture'] == 'home':\n",
    "            if home_start_time is None:\n",
    "                home_start_time = time.time()\n",
    "            else:\n",
    "                elapsed_time = time.time() - home_start_time\n",
    "                if elapsed_time >= 3:  # 3초 이상 동안 'home' 제스처가 인식되면\n",
    "                    engine.say('안내를 시작합니다')\n",
    "                    engine.runAndWait()\n",
    "                    home_start_time = None  # 초기화\n",
    "                    route()\n",
    "                    \n",
    "        else:\n",
    "            home_start_time = None  # 'home' 제스처가 아닌 경우 초기화\n",
    "\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # 키포인트 표시\n",
    "    img_2d = disp.draw2d(img.copy(), param)\n",
    "\n",
    "    # dlib 얼굴 및 랜드마크 검출\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        face = faces[0]\n",
    "        shapes = predictor(gray, face)\n",
    "        shapes = face_utils.shape_to_np(shapes)\n",
    "\n",
    "        eye_img_l, eye_rect_l = crop_eye(gray, eye_points=shapes[36:42])\n",
    "        eye_img_r, eye_rect_r = crop_eye(gray, eye_points=shapes[42:48])\n",
    "\n",
    "        eye_img_l = cv2.resize(eye_img_l, dsize=IMG_SIZE)\n",
    "        eye_img_r = cv2.resize(eye_img_r, dsize=IMG_SIZE)\n",
    "\n",
    "        # 눈 이미지 수평으로 뒤집기\n",
    "        eye_img_l_flipped = cv2.flip(eye_img_l, 1)\n",
    "        eye_img_r_flipped = cv2.flip(eye_img_r, 1)\n",
    "\n",
    "        eye_input_l = eye_img_l.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "        eye_input_r = eye_img_r.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n",
    "\n",
    "        pred_l = model.predict(eye_input_l)\n",
    "        pred_r = model.predict(eye_input_r)\n",
    "\n",
    "        # 눈 깜빡임 주기 확인\n",
    "        if pred_l < 0.2 and pred_r < 0.2:\n",
    "            blink_counter += 1\n",
    "            if blink_counter >= blink_threshold:\n",
    "                engine.say(\"졸음 운전 감지\")\n",
    "                engine.runAndWait()\n",
    "                blink_counter = 0  # 초기화\n",
    "        else:\n",
    "            blink_counter = 0  # 초기화\n",
    "\n",
    "        # visualize\n",
    "        state_l = 'O %.1f' if pred_l > 0.1 else '- %.1f'\n",
    "        state_r = 'O %.1f' if pred_r > 0.1 else '- %.1f'\n",
    "\n",
    "        state_l = state_l % pred_l\n",
    "        state_r = state_r % pred_r\n",
    "\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_l[0:2]), pt2=tuple(eye_rect_l[2:4]), color=(255, 255, 255), thickness=2)\n",
    "        cv2.rectangle(img, pt1=tuple(eye_rect_r[0:2]), pt2=tuple(eye_rect_r[2:4]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "        cv2.putText(img, state_l, tuple(eye_rect_l[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img, state_r, tuple(eye_rect_r[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    # 이미지 합치기\n",
    "    combined_img = cv2.addWeighted(img_2d, 0.5, img, 0.5, 0)\n",
    "\n",
    "    cv2.imshow('result', combined_img)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "engine.stop()\n",
    "pipe.pipe.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
